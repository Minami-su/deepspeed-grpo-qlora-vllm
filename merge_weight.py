import argparse
import os
from safetensors import safe_open
import torch
from transformers import AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, TaskType

# --- 1. å‚æ•°å®šä¹‰ ---
parser = argparse.ArgumentParser(
    description="Precisely convert custom LoRA weights to the PEFT format, handling special embedding layers."
)
parser.add_argument("--input_model", default="/data/jcxy/llm_model/PsyLLM-4.0-Turbo-09-13-E-I2", type=str, help="Path to the base Hugging Face model.")
parser.add_argument("--input_lora", default="/data/jcxy/haolu/workspace/lora/PsyLLM-4.0-Turbo-09-13-E-I2-Adult-Children-DPO/checkpoint-200-weight", type=str, help="Path to the directory containing custom LoRA weights (model.safetensors).")
parser.add_argument("--output_lora", default="/data/jcxy/haolu/workspace/lora/PsyLLM-4.0-Turbo-09-13-E-I2-Adult-Children-DPO/checkpoint-200-weight-peft", type=str, help="Path to save the converted LoRA in PEFT format.")
args = parser.parse_args()

# --- 2. åŠ è½½è‡ªå®šä¹‰ LoRA æƒé‡ ---
print(f"Loading custom LoRA weights from: {args.input_lora}")
tensors = {}
lora_file_path = os.path.join(args.input_lora, "model.safetensors")
if not os.path.exists(lora_file_path):
    raise FileNotFoundError(f"LoRA weights file not found at: {lora_file_path}")

with safe_open(lora_file_path, framework="pt", device="cpu") as f:
    for k in f.keys():
        tensors[k] = f.get_tensor(k)
print(f"Loaded {len(tensors)} tensors from custom LoRA file.")

# --- 3. åŠ è½½åŸºç¡€æ¨¡åž‹å¹¶åº”ç”¨ PEFT é…ç½® ---
print(f"Loading base model from: {args.input_model}")
model = AutoModelForCausalLM.from_pretrained(
    args.input_model,
    use_cache=False,
    low_cpu_mem_usage=True,
    torch_dtype=torch.bfloat16,
    device_map={"": "cpu"},
)

for param in model.parameters():
    param.requires_grad = False

target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "gate_proj", "down_proj", "lm_head", "embed_tokens"]
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, 
    r=128, 
    lora_alpha=256, 
    lora_dropout=0.0,
    bias="none",
    target_modules=target_modules
)

print("Applying PEFT configuration...")
model = get_peft_model(model, peft_config)

# --- 4. æ ¸å¿ƒé€»è¾‘ï¼šç²¾ç¡®çš„æƒé‡æ˜ å°„ (ä¿®æ­£ç‰ˆ) ---
new_sd = model.state_dict()
mismatched_keys = []
mapped_keys_count = 0

# åŠ¨æ€ç¡®å®šåŸºç¡€å‰ç¼€ï¼Œä»¥å¤„ç†å‘½åä¸ä¸€è‡´é—®é¢˜
# æˆ‘ä»¬å‡è®¾æºå’Œç›®æ ‡è‡³å°‘éƒ½æœ‰ 'base_model.model'
source_base_prefix = "base_model.model."
if not any(k.startswith(source_base_prefix) for k in tensors.keys()):
    source_base_prefix = "" # å¦‚æžœæºæ²¡æœ‰è¿™ä¸ªå‰ç¼€ï¼Œå°±ç½®ç©º

peft_base_prefix = "base_model.model.model."
if not any(k.startswith(peft_base_prefix) for k in new_sd.keys()):
    # å¦‚æžœ PEFT æ¨¡åž‹çš„å‰ç¼€ä¸åŒï¼Œæ‰¾åˆ°å®žé™…çš„å…¬å…±å‰ç¼€
    peft_base_prefix = os.path.commonprefix([k for k in new_sd.keys() if 'lora' in k]) or ""


print("\nStarting LoRA weight mapping process...")
for peft_key in new_sd:
    if 'lora' not in peft_key:
        continue
    
    # ä»Ž PEFT key ä¸­ç§»é™¤å…¶ç‹¬æœ‰çš„å‰ç¼€å’ŒåŽç¼€ï¼Œå¾—åˆ°ä¸€ä¸ªå¹²å‡€çš„ã€å¯è¯†åˆ«çš„æ¨¡å—è·¯å¾„
    clean_peft_path = peft_key.replace(peft_base_prefix, "")
    clean_peft_path = clean_peft_path.replace('.default.weight', '').replace('.default', '')
    
    # æ ¹æ®è¿™ä¸ªå¹²å‡€è·¯å¾„ï¼Œæž„é€ æˆ‘ä»¬æœŸæœ›åœ¨æºæ–‡ä»¶ä¸­æ‰¾åˆ°çš„é”®å
    source_key = None
    
    # --- å…³é”®ä¿®å¤ï¼šä¸“é—¨å¤„ç† embed_tokens çš„å‘½åè§„åˆ™ ---
    if clean_peft_path.startswith('embed_tokens'):
        # æœŸæœ›çš„æºé”®æ²¡æœ‰ .weight åŽç¼€
        source_key = source_base_prefix + clean_peft_path
            
    # å…¶ä»–æ‰€æœ‰å±‚ï¼Œéƒ½åº”è¯¥æœ‰ .weight åŽç¼€
    else:
        source_key = source_base_prefix + clean_peft_path + '.weight'
            
    # åŠ è½½æƒé‡æˆ–æŠ¥å‘Šä¸åŒ¹é…
    if source_key and source_key in tensors:
        print(f"  âœ… Mapping [Source] {source_key}  ->  [Target] {peft_key}")
        target_tensor = new_sd[peft_key]
        
        source_tensor = tensors[source_key]
        # ç¡®ä¿æ•°æ®ç±»åž‹å’Œè®¾å¤‡åŒ¹é…
        new_sd[peft_key] = source_tensor.to(target_tensor.device, target_tensor.dtype)
        
        # æ£€æŸ¥å¹¶è°ƒæ•´æƒé‡å½¢çŠ¶ (å¦‚æžœéœ€è¦)
        if new_sd[peft_key].shape != source_tensor.shape:
             print(f"     âš ï¸ Shape mismatch! Target: {new_sd[peft_key].shape}, Source: {source_tensor.shape}. This should not happen if r and alpha match.")

        mapped_keys_count += 1
    else:
        mismatched_keys.append((peft_key, source_key or f"Pattern not found for {peft_key}"))

# åŠ è½½ state_dict
model.load_state_dict(new_sd, strict=False)


# --- 5. æŠ¥å‘Šå’Œä¿å­˜ ---
print("\n" + "="*50)
print("LoRA Weight Mapping Report")
print("="*50)
total_lora_keys_in_model = len([k for k in new_sd if 'lora' in k])
print(f"Total LoRA parameters in PEFT model: {total_lora_keys_in_model}")
print(f"Successfully mapped parameters: {mapped_keys_count}")

if mismatched_keys:
    print(f"\nðŸš¨ Unmapped parameters: {len(mismatched_keys)}")
    print("PEFT Key -> Tried to map from Source Key:")
    for peft_key, tried_key in mismatched_keys:
        print(f"  - {peft_key} -> {tried_key}")
else:
    print("\nâœ… All LoRA parameters were successfully mapped!")

print("="*50)

if mapped_keys_count < total_lora_keys_in_model:
    print("\n\nâš ï¸ WARNING: Some LoRA weights were not mapped. Unmapped layers will have random initialization.")

print(f"\nSaving converted model to PEFT format at: {args.output_lora}")
os.makedirs(args.output_lora, exist_ok=True)
model.save_pretrained(args.output_lora)

adapter_config_path = os.path.join(args.output_lora, "adapter_config.json")
if not os.path.exists(adapter_config_path):
    peft_config.save_pretrained(args.output_lora)
    
print("\nConversion complete! âœ¨")