2025-07-11 18:57:55.968 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:91 - Raw dataset: ['text']
2025-07-11 18:57:55.968 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:107 - Dataset after column renaming: ['text']
2025-07-11 18:57:55.968 | DEBUG    | llmcompressor.transformers.finetune.data.base:filter_tokenizer_args:239 - Found processor args `{'return_attention_mask', 'padding', 'add_special_tokens', 'padding_side', 'text_target', 'return_tensors', 'truncation', 'return_offsets_mapping', 'stride', 'text_pair_target', 'pad_to_multiple_of', 'return_special_tokens_mask', 'text', 'return_overflowing_tokens', 'return_token_type_ids', 'verbose', 'is_split_into_words', 'return_length', 'text_pair', 'max_length'}`. Removing all other columns
2025-07-11 18:57:55.972 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:114 - Tokenizer args after filtering: ['text']
2025-07-11 18:58:03.529 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:128 - Model kwargs after tokenizing: ['input_ids', 'attention_mask']
2025-07-11 18:58:03.530 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:158 - Model kwargs after postprocessing: ['input_ids', 'attention_mask']
2025-07-11 18:58:03.534 | DEBUG    | llmcompressor.core.lifecycle:reset:61 - Resetting compression lifecycle
2025-07-11 18:58:03.535 | INFO     | llmcompressor.core.lifecycle:reset:73 - Compression lifecycle reset
2025-07-11 18:58:03.535 | DEBUG    | llmcompressor.core.state:update:182 - Updating state with provided parameters: {'model': Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(151936, 896)
    (layers): ModuleList(
      (0-23): 24 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): Linear(in_features=896, out_features=896, bias=True)
          (k_proj): Linear(in_features=896, out_features=128, bias=True)
          (v_proj): Linear(in_features=896, out_features=128, bias=True)
          (o_proj): Linear(in_features=896, out_features=896, bias=False)
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
          (up_proj): Linear(in_features=896, out_features=4864, bias=False)
          (down_proj): Linear(in_features=4864, out_features=896, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((896,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=896, out_features=151936, bias=False)
), 'teacher_model': None, 'optimizer': None, 'attach_optim_callbacks': True, 'train_data': None, 'val_data': None, 'test_data': None, 'calib_data': <torch.utils.data.dataloader.DataLoader object at 0x7efcd67701c0>, 'copy_data': True, 'start': -1, 'steps_per_epoch': None, 'batches_per_step': None, 'loggers': None, 'model_log_cadence': None, 'kwargs': {}}
2025-07-11 18:58:03.539 | DEBUG    | llmcompressor.core.lifecycle:initialize:94 - Initializing compression lifecycle
2025-07-11 18:58:03.539 | INFO     | llmcompressor.recipe.recipe:from_modifiers:59 - Creating recipe from modifiers
2025-07-11 18:58:04.219 | INFO     | llmcompressor.modifiers.awq.base:on_initialize:222 - No AWQModifier.mappings provided, inferring from model...
2025-07-11 18:58:04.293 | DEBUG    | llmcompressor.core.lifecycle:initialize:103 - Initialized modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=False, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='default' applied=False
2025-07-11 18:58:04.293 | INFO     | llmcompressor.core.lifecycle:initialize:108 - Compression lifecycle initialized for 1 modifiers
2025-07-11 18:58:04.293 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:47 - Inferred `SequentialPipeline` for `AWQModifier`
2025-07-11 18:58:05.180 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.180 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if labels is not None:
    loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
2025-07-11 18:58:05.180 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.184 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.184 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if (input_ids is None) ^ (inputs_embeds is not None):
    raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
2025-07-11 18:58:05.184 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.184 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.184 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if not isinstance(past_key_values, (type(None), Cache)):
    raise ValueError('The `past_key_values` should be either a `Cache` object or `None`.')
2025-07-11 18:58:05.184 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.184 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.184 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if inputs_embeds is None:
    inputs_embeds = self.embed_tokens(input_ids)
2025-07-11 18:58:05.184 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if use_cache and past_key_values is None:
    past_key_values = DynamicCache()
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if cache_position is None:
    past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
    cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if position_ids is None:
    position_ids = cache_position.unsqueeze(0)
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:189 - ---- Autowrapper ----
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:190 - self._update_causal_mask(attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions)
2025-07-11 18:58:05.185 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:191 - ---------------------
2025-07-11 18:58:05.186 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.186 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if output_hidden_states:
    all_hidden_states += (hidden_states,)
2025-07-11 18:58:05.186 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.186 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.186 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if output_attentions:
    all_self_attns += (layer_outputs[1],)
2025-07-11 18:58:05.186 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.186 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:183 - ---- Autowrapper ----
2025-07-11 18:58:05.186 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:184 - if output_hidden_states:
    all_hidden_states += (hidden_states,)
2025-07-11 18:58:05.186 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_if_possible:185 - ---------------------
2025-07-11 18:58:05.280 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.CALIBRATION_EPOCH_START
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efdc624cdf0>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcdddadf90>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcdddae200>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcdddad600>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcdddad690>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcdddad9f0>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd674fc10>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd674e500>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd674c310>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd674eda0>
2025-07-11 18:58:05.285 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2aef0>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2a590>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2aaa0>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2b760>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2b5b0>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf28220>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2baf0>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2a770>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2b010>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2ad10>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2bf40>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2bbe0>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2be80>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2bfd0>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2ba60>
2025-07-11 18:58:05.286 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6770d30>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6771fc0>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6773730>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6772c50>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd67737f0>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd67705b0>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6771b40>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6771510>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6771390>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6772650>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6773790>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6773520>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6772cb0>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6772080>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6771900>
2025-07-11 18:58:05.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd6771b70>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7efcd67718a0>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2f340>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2f370>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d450>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2ed40>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2e260>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2dff0>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d930>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2dab0>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d060>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d1b0>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2c430>
2025-07-11 18:58:05.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2c0d0>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2fd00>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2fd30>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2c8b0>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2f9d0>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2f640>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2f580>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2e650>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2c100>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2ce80>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2f460>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2e4d0>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2df90>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2da50>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2ffd0>
2025-07-11 18:58:05.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2e920>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2ece0>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2f6a0>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2efe0>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2ef80>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2eb60>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2ec20>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2da20>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d8a0>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2dbd0>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2e410>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2dfc0>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2de40>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d480>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2e980>
2025-07-11 18:58:05.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d090>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d0f0>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2c9a0>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2c880>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2ff40>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2fd90>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d600>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2f6d0>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2d4b0>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2eb90>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2cb80>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3bf2c040>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718c0a0>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718c160>
2025-07-11 18:58:05.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718c2e0>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718c3a0>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718c520>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718c5e0>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718c760>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718c820>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718c9a0>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718ca60>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718cbe0>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718cca0>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718ce20>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718cee0>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718d060>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718d120>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718d2a0>
2025-07-11 18:58:05.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718d360>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718d4e0>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718d5a0>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718d720>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718d7e0>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718d960>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718da20>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718dba0>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718dc60>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718dde0>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718dea0>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e020>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e0e0>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e260>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e320>
2025-07-11 18:58:05.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e4a0>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e560>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e6e0>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e7a0>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e920>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718e9e0>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718eb60>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718ec20>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718eda0>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718ee60>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718efe0>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718f0a0>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718f220>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718f2e0>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718f460>
2025-07-11 18:58:05.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718f520>
2025-07-11 18:58:05.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16' kv_cache_scheme=None index=0 group='default' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7eff3718f6a0>
2025-07-11 18:58:05.295 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:58:11.571 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:58:18.264 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:58:24.115 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:58:28.348 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:58:33.503 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:58:38.170 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:58:43.614 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:58:47.980 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:58:53.198 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:58:57.679 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:59:03.101 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:59:07.473 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:59:12.743 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:59:17.192 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:59:22.497 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:59:26.979 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:59:32.326 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:59:36.731 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:59:42.043 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:59:46.476 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 18:59:51.927 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 18:59:56.273 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:00:01.754 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:00:06.198 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:00:11.746 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:00:16.056 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:00:21.372 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:00:25.774 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:00:31.121 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:00:35.509 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:00:40.994 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:00:45.345 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:00:50.638 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:00:55.076 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:01:00.285 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:01:04.517 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:01:10.112 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:01:14.537 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:01:20.084 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:01:24.584 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:01:29.755 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:01:34.265 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:01:39.471 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:01:43.553 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:01:49.226 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:01:53.711 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:01:58.846 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:02:03.440 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:02:05.920 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-07-11 19:02:05.921 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=False, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:02:06.487 | DEBUG    | llmcompressor.core.lifecycle:event:191 - Handling event: EventType.CALIBRATION_EPOCH_END
2025-07-11 19:02:09.326 | DEBUG    | llmcompressor.core.lifecycle:event:201 - Updated event with modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=False, started_=True, ended_=True, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='AWQModifier' applied=False
2025-07-11 19:02:09.330 | DEBUG    | llmcompressor.core.lifecycle:finalize:131 - Finalizing compression lifecycle
2025-07-11 19:02:09.330 | DEBUG    | llmcompressor.core.lifecycle:finalize:135 - Finalized modifier: modifiers=[AWQModifier(config_groups=None, targets=['Linear'], ignore=['lm_head'], scheme='W4A16', kv_cache_scheme=None, index=0, group='default', start=None, end=None, update=None, initialized_=True, finalized_=True, started_=True, ended_=True, sequential_targets=None, mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])], offload_device=None, duo_scaling=True)] index=0 group='default' applied=True
2025-07-11 19:02:09.330 | INFO     | llmcompressor.core.lifecycle:finalize:141 - Compression lifecycle finalized for 1 modifiers
2025-07-11 19:02:09.678 | WARNING  | llmcompressor.entrypoints.utils:post_process:107 - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`
2025-07-11 19:02:09.718 | INFO     | llmcompressor.transformers.sparsification.compressed_tensors_utils:get_model_compressor:217 - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.
2025-07-11 19:02:14.085 | DEBUG    | llmcompressor.transformers.utils.helpers:infer_recipe_from_model_path:105 - No recipe found in the model_path: /data2/jcxy/llm_model/Qwen2.5-0.5B
