  0%|          | 0/375 [00:00<?, ?it/s]
Moving LoRA weights to vLLM server (using summon_full_params)...
Found and sending LoRA param: .model.embed_tokens.lora_embedding_A.default, shape: torch.Size([128, 151936])
Found and sending LoRA param: .model.embed_tokens.lora_embedding_B.default, shape: torch.Size([896, 128])
Found and sending LoRA param: .model.layers.0.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.0.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.0.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.0.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.0.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.0.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.1.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.1.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.1.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.1.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.1.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.2.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.2.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.2.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.2.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.2.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.3.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.3.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.3.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.3.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.3.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.4.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.4.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.4.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.4.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.4.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.5.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.5.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.5.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.5.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.5.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.6.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.6.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.6.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.6.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.6.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.7.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.7.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.7.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.7.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.7.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.8.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.8.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.8.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.8.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.8.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.9.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.9.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.9.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.9.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.9.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.10.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.10.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.10.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.10.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.10.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.11.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.11.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.11.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.11.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.11.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.12.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.12.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.12.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.12.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.12.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.13.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.13.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.13.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.13.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.13.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.14.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.14.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.14.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.14.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.14.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.15.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.15.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.15.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.15.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.15.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.16.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.16.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.16.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.16.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.16.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.17.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.17.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.17.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.17.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.17.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.18.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.18.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.18.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.18.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.18.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.19.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.19.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.19.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.19.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.19.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.20.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.20.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.20.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.20.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.20.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.21.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.21.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.21.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.21.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.21.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.22.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.22.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.22.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.22.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.22.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.23.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.23.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.23.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.23.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.23.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .lm_head.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .lm_head.lora_B.default.weight, shape: torch.Size([6482603])
Found and sending LoRA param: _fsdp_wrapped_module.model.embed_tokens.lora_embedding_A.default, shape: torch.Size([0])
Found and sending LoRA param: _fsdp_wrapped_module.model.embed_tokens.lora_embedding_B.default, shape: torch.Size([0])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.0._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.1._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.2._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.3._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.4._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.5._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.6._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.7._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.8._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.9._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.10._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.11._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.12._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.13._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.14._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.15._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.16._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.17._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.18._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.19._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.20._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.21._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.22._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.q_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.k_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.v_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([5462])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.self_attn.o_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.gate_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.up_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([207531])
Found and sending LoRA param: _fsdp_wrapped_module.model.layers.23._fsdp_wrapped_module.mlp.down_proj.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.lm_head.lora_A.default._fsdp_wrapped_module.weight, shape: torch.Size([38230])
Found and sending LoRA param: _fsdp_wrapped_module.lm_head.lora_B.default._fsdp_wrapped_module.weight, shape: torch.Size([6482603])
All LoRA parameters sent. Applying adapter on vLLM server...
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/sgrpo.py", line 294, in <module>
    main(script_args, training_args, model_args )
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/sgrpo.py", line 258, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 3730, in training_step
    inputs = self._prepare_inputs(inputs)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1183, in _prepare_inputs
    generation_batch = self._generate_and_score_completions(generation_batch)
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1276, in _generate_and_score_completions
    self._move_lora_to_vllm()
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1156, in _move_lora_to_vllm
    self.vllm_client.apply_lora(self.lora_config.to_dict())
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/vllm_client.py", line 320, in apply_lora
    config_dict = config.to_dict()
AttributeError: 'dict' object has no attribute 'to_dict'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/sgrpo.py", line 294, in <module>
[rank0]:     main(script_args, training_args, model_args )
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/sgrpo.py", line 258, in main
[rank0]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 3730, in training_step
[rank0]:     inputs = self._prepare_inputs(inputs)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
[rank0]:     return func(self, *args, **kwargs)
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1183, in _prepare_inputs
[rank0]:     generation_batch = self._generate_and_score_completions(generation_batch)
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1276, in _generate_and_score_completions
[rank0]:     self._move_lora_to_vllm()
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
[rank0]:     return func(self, *args, **kwargs)
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1156, in _move_lora_to_vllm
[rank0]:     self.vllm_client.apply_lora(self.lora_config.to_dict())
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/vllm_client.py", line 320, in apply_lora
[rank0]:     config_dict = config.to_dict()
[rank0]: AttributeError: 'dict' object has no attribute 'to_dict'
