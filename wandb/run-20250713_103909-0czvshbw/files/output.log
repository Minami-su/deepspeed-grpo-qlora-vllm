  0%|          | 0/375 [00:00<?, ?it/s]
Moving LoRA weights to vLLM server (using summon_full_params)...
Found and sending LoRA param: .model.layers.0.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.0.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.0.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.0.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.0.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.0.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.0.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.1.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.1.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.1.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.1.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.1.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.1.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.2.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.2.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.2.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.2.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.2.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.2.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.3.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.3.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.3.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.3.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.3.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.3.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.4.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.4.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.4.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.4.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.4.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.4.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.5.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.5.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.5.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.5.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.5.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.5.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.6.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.6.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.6.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.6.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.6.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.6.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.7.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.7.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.7.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.7.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.7.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.7.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.8.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.8.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.8.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.8.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.8.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.8.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.9.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.9.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.9.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.9.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.9.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.9.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.10.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.10.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.10.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.10.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.10.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.10.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.11.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.11.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.11.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.11.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.11.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.11.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.12.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.12.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.12.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.12.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.12.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.12.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.13.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.13.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.13.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.13.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.13.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.13.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.14.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.14.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.14.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.14.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.14.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.14.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.15.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.15.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.15.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.15.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.15.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.15.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.16.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.16.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.16.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.16.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.16.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.16.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.17.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.17.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.17.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.17.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.17.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.17.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.18.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.18.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.18.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.18.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.18.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.18.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.19.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.19.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.19.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.19.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.19.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.19.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.20.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.20.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.20.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.20.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.20.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.20.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.21.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.21.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.21.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.21.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.21.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.21.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.22.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.22.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.22.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.22.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.22.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.22.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.q_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.q_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.k_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.k_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.23.self_attn.v_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.v_proj.lora_B.default.weight, shape: torch.Size([5462])
Found and sending LoRA param: .model.layers.23.self_attn.o_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.self_attn.o_proj.lora_B.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.mlp.gate_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.mlp.gate_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.23.mlp.up_proj.lora_A.default.weight, shape: torch.Size([38230])
Found and sending LoRA param: .model.layers.23.mlp.up_proj.lora_B.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.23.mlp.down_proj.lora_A.default.weight, shape: torch.Size([207531])
Found and sending LoRA param: .model.layers.23.mlp.down_proj.lora_B.default.weight, shape: torch.Size([38230])
All LoRA parameters sent. Applying adapter on vLLM server...
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/sgrpo.py", line 294, in <module>
    main(script_args, training_args, model_args )
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/sgrpo.py", line 258, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 3730, in training_step
    inputs = self._prepare_inputs(inputs)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1226, in _prepare_inputs
    mode = "train" if self.model.training else "eval"
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1319, in _generate_and_score_completions
    # Generate completions using either vLLM or regular generation
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1160, in _move_lora_to_vllm
    if hasattr(self, "lora_config") and self.lora_config:
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/vllm_client.py", line 334, in reset_prefix_cache
    response = self.session.post(url)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/http/client.py", line 1375, in getresponse
    response.begin()
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/http/client.py", line 318, in begin
    version, status, reason = self._read_status()
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/http/client.py", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/sgrpo.py", line 294, in <module>
[rank0]:     main(script_args, training_args, model_args )
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/sgrpo.py", line 258, in main
[rank0]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/transformers/trainer.py", line 3730, in training_step
[rank0]:     inputs = self._prepare_inputs(inputs)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
[rank0]:     return func(self, *args, **kwargs)
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1226, in _prepare_inputs
[rank0]:     mode = "train" if self.model.training else "eval"
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1319, in _generate_and_score_completions
[rank0]:     # Generate completions using either vLLM or regular generation
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
[rank0]:     return func(self, *args, **kwargs)
[rank0]:   File "/data/jcxy/haolu/workspace/frameworks/X-R1/src/x_r1/s_grpo_trainer.py", line 1160, in _move_lora_to_vllm
[rank0]:     if hasattr(self, "lora_config") and self.lora_config:
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/trl/extras/vllm_client.py", line 334, in reset_prefix_cache
[rank0]:     response = self.session.post(url)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/requests/sessions.py", line 637, in post
[rank0]:     return self.request("POST", url, data=data, json=json, **kwargs)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
[rank0]:     resp = self.send(prep, **send_kwargs)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
[rank0]:     r = adapter.send(request, **kwargs)
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
[rank0]:     resp = conn.urlopen(
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
[rank0]:     response = self._make_request(
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/urllib3/connectionpool.py", line 534, in _make_request
[rank0]:     response = conn.getresponse()
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/urllib3/connection.py", line 516, in getresponse
[rank0]:     httplib_response = super().getresponse()
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/http/client.py", line 1375, in getresponse
[rank0]:     response.begin()
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/http/client.py", line 318, in begin
[rank0]:     version, status, reason = self._read_status()
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/http/client.py", line 279, in _read_status
[rank0]:     line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
[rank0]:   File "/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/socket.py", line 705, in readinto
[rank0]:     return self._sock.recv_into(b)
[rank0]: KeyboardInterrupt
