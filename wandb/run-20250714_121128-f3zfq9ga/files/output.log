  0%|          | 0/375 [00:00<?, ?it/s]/data/jcxy/haolu/anaconda3/envs/haolu/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
Process 0 finished weight gathering, waiting at barrier...
Process 0 passed the first barrier.
Main process is now sending weights to vLLM server...
Moving LoRA weights to target device: cuda:0
All LoRA parameters sent. Applying adapter on vLLM server...
Main process finished all communication with vLLM server.
Process 0 waiting at the final barrier...
Process 0 passed the final barrier. _move_lora_to_vllm complete.
okkkkkkkkkk!1
okkkkkkkkkk!2
okkkkkkkkkk!3
okkkkkkkkkk!4
okkkkkkkkkk!5
